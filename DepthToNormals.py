import torch
import torch.nn.functional as F
import cv2
import numpy as np


class DepthToNormals:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "depth": ("IMAGE", {
                    "tooltip": "è¾“å…¥çš„æ·±åº¦å›¾åƒï¼Œæ·±åº¦å€¼å°†è¢«è½¬æ¢ä¸ºæ³•çº¿è´´å›¾"
                }),
                "scale": ("FLOAT",{"default": 0.1, "min": 0.001, "max": 1000, "step": 0.001,
                                  "tooltip": "æ·±åº¦ç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶æ³•çº¿çš„å¼ºåº¦å’Œå‡¸å‡¹ç¨‹åº¦ã€‚å€¼è¶Šå¤§ï¼Œè¡¨é¢èµ·ä¼è¶Šæ˜æ˜¾"}),
                "output_mode": (["Standard", "BAE", "MiDaS"], {
                    "tooltip": "æ³•çº¿è´´å›¾è¾“å‡ºæ¨¡å¼ï¼šStandard=æ ‡å‡†æ¨¡å¼ï¼ŒBAE=ç¿»è½¬Xè½´ï¼ŒMiDaS=ç¿»è½¬XYè½´é¡ºåº"
                }),
                "denoise_method": (["none", "gentle_blur", "nlmeans_light", "bilateral_light"], {
                    "tooltip": "æ·±åº¦å»å™ªæ–¹æ³•ï¼šnone=ä¸å»å™ªï¼Œgentle_blur=è½»åº¦é«˜æ–¯æ¨¡ç³Šï¼Œnlmeans_light=è½»åº¦éå±€éƒ¨å‡å€¼ï¼Œbilateral_light=è½»åº¦åŒè¾¹æ»¤æ³¢"
                }),
                "denoise_strength": ("FLOAT", {"default": 0.3, "min": 0.05, "max": 2.0, "step": 0.05,
                    "tooltip": "å»å™ªå¼ºåº¦ã€‚å»ºè®®0.1-0.5ç”¨äºè½»åº¦å»å™ªï¼Œä¿æŒç»†èŠ‚"}),
                "detail_preservation": ("FLOAT", {"default": 0.95, "min": 0.7, "max": 1.0, "step": 0.05,
                    "tooltip": "ç»†èŠ‚ä¿æŠ¤å¼ºåº¦ã€‚å€¼è¶Šé«˜è¶Šèƒ½ä¿æŠ¤ç»†èŠ‚ï¼Œå»ºè®®0.9ä»¥ä¸Š"}),
                "noise_threshold": ("FLOAT", {"default": 0.02, "min": 0.005, "max": 0.1, "step": 0.005,
                    "tooltip": "å™ªå£°æ£€æµ‹é˜ˆå€¼ã€‚åªå¤„ç†å˜åŒ–å°äºæ­¤å€¼çš„åŒºåŸŸï¼Œé¿å…ç ´åçœŸå®ç»†èŠ‚"}),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("normals",)
    FUNCTION = "normal_map"

    CATEGORY = "image/filters"

    def gentle_denoise_depth(self, depth, method, strength, detail_preservation, noise_threshold):
        """æ¸©å’Œçš„æ·±åº¦å›¾å»å™ªå¤„ç†"""
        if method == "none":
            return depth
            
        # åªå¤„ç†æ·±åº¦é€šé“ï¼ˆç¬¬3ä¸ªé€šé“ï¼‰
        depth_channel = depth[..., 2]  # æå–æ·±åº¦é€šé“ [B,H,W]
        
        if method == "gentle_blur":
            # ä½¿ç”¨PyTorchå®ç°è½»åº¦é«˜æ–¯æ¨¡ç³Š
            kernel_size = max(3, int(strength * 10) | 1)  # ç¡®ä¿å¥‡æ•°
            sigma = strength * 0.5
            
            # åˆ›å»ºé«˜æ–¯æ ¸
            k = kernel_size // 2
            ax = torch.arange(-k, k + 1, dtype=torch.float32, device=depth.device)
            xx, yy = torch.meshgrid(ax, ax, indexing='ij')
            kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))
            kernel = kernel / kernel.sum()
            kernel = kernel.view(1, 1, kernel_size, kernel_size)
            
            # åº”ç”¨é«˜æ–¯æ¨¡ç³Š
            depth_blurred = F.conv2d(depth_channel.unsqueeze(1), kernel, padding=k)
            depth_blurred = depth_blurred.squeeze(1)
            
            # æ ¹æ®detail_preservationæ··åˆåŸå§‹å’Œæ¨¡ç³Šç‰ˆæœ¬
            alpha = 1 - detail_preservation
            denoised_tensor = depth_channel * (1 - alpha) + depth_blurred * alpha
            
        else:
            # å¯¹äºcv2æ–¹æ³•ï¼Œä½¿ç”¨æå…¶æ¸©å’Œçš„å‚æ•°
            depth_np = depth_channel.cpu().numpy()
            batch_size = depth_np.shape[0]
            
            denoised_batch = []
            for i in range(batch_size):
                depth_single = depth_np[i]
                # è½¬æ¢ä¸ºuint8ç”¨äºcv2å¤„ç†
                depth_uint8 = (depth_single * 255).astype(np.uint8)
                
                if method == "nlmeans_light":
                    # æå…¶æ¸©å’Œçš„éå±€éƒ¨å‡å€¼å‚æ•°
                    h = max(0.5, strength * 2)  # æœ€å°0.5ï¼Œæœ€å¤§4
                    template_window = 3  # å›ºå®šæœ€å°çª—å£
                    search_window = 7    # å›ºå®šæœ€å°æœç´¢çª—å£
                    
                    denoised = cv2.fastNlMeansDenoising(depth_uint8, 
                                                       None,
                                                       h=h,
                                                       templateWindowSize=template_window,
                                                       searchWindowSize=search_window)
                    
                elif method == "bilateral_light":
                    # æå…¶æ¸©å’Œçš„åŒè¾¹æ»¤æ³¢å‚æ•°
                    d = 3  # å›ºå®šæœ€å°ç›´å¾„
                    sigma_color = max(5, strength * 15)  # æœ€å°5ï¼Œæœ€å¤§30
                    sigma_space = max(5, strength * 15)
                    
                    denoised = cv2.bilateralFilter(depth_uint8, d, sigma_color, sigma_space)
                    
                # è½¬å›floatå¹¶å½’ä¸€åŒ–
                denoised_float = denoised.astype(np.float32) / 255.0
                
                # æ ¹æ®detail_preservationæ§åˆ¶å»å™ªå¼ºåº¦
                original = depth_single
                alpha = 1 - detail_preservation
                final_denoised = original * (1 - alpha) + denoised_float * alpha
                
                denoised_batch.append(final_denoised)
            
            # è½¬å›torch tensor
            denoised_tensor = torch.from_numpy(np.stack(denoised_batch)).to(depth.device)
        
        # æ™ºèƒ½å™ªå£°æ£€æµ‹ï¼šåªåœ¨å˜åŒ–å°çš„åŒºåŸŸåº”ç”¨å»å™ª
        if noise_threshold > 0:
            # è®¡ç®—å±€éƒ¨æ¢¯åº¦å¹…åº¦
            grad_x = torch.diff(depth_channel, dim=2, prepend=depth_channel[:, :, :1])
            grad_y = torch.diff(depth_channel, dim=1, prepend=depth_channel[:, :1, :])
            grad_magnitude = torch.sqrt(grad_x**2 + grad_y**2)
            
            # åˆ›å»ºå™ªå£°æ©ç ï¼šåªæœ‰æ¢¯åº¦å°çš„åŒºåŸŸè¢«è®¤ä¸ºæ˜¯å™ªå£°
            noise_mask = (grad_magnitude < noise_threshold).float()
            
            # å¹³æ»‘æ©ç è¾¹ç•Œ
            noise_mask = F.conv2d(noise_mask.unsqueeze(1), 
                                torch.ones(1, 1, 3, 3, device=depth.device) / 9, 
                                padding=1).squeeze(1)
            
            # æ ¹æ®æ©ç æ··åˆåŸå§‹å’Œå»å™ªç‰ˆæœ¬
            denoised_tensor = depth_channel * (1 - noise_mask) + denoised_tensor * noise_mask
        
        # æ›¿æ¢æ·±åº¦é€šé“
        result = depth.clone()
        result[..., 2] = denoised_tensor
        
        return result

    def normal_map(self, depth, scale, output_mode, denoise_method="gentle_blur", 
                   denoise_strength=0.3, detail_preservation=0.95, noise_threshold=0.02):
        
        device = depth.device
        
        # æ ‡å‡†3x3 Sobelæ ¸ï¼ˆæ¸…æ™°æ— æ¨¡ç³Šï¼‰
        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                               dtype=torch.float32, device=device)
        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                               dtype=torch.float32, device=device)
        kernel_x = kernel_x.unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1)
        kernel_y = kernel_y.unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1)
        
        conv2d = F.conv2d
        pad = F.pad
        
        size_x = depth.size(2)
        size_y = depth.size(1)
        max_dim = max(size_x, size_y)
        
        # æ„å»º3Dä½ç½®å›¾
        position_map = depth.detach().clone() * scale
        xs = torch.linspace(-1 * size_x / max_dim, 1 * size_x / max_dim, steps=size_x, device=device)
        ys = torch.linspace(-1 * size_y / max_dim, 1 * size_y / max_dim, steps=size_y, device=device)
        grid_x, grid_y = torch.meshgrid(xs, ys, indexing='xy')
        position_map[..., 0] = grid_x.unsqueeze(0)
        position_map[..., 1] = grid_y.unsqueeze(0)
        
        # ğŸ¯ æ¸©å’Œçš„æ·±åº¦å»å™ª
        if denoise_method != "none":
            position_map = self.gentle_denoise_depth(position_map, denoise_method, 
                                                   denoise_strength, detail_preservation, noise_threshold)
        
        position_map = position_map.movedim(-1, 1) # BCHW
        
        # ä½¿ç”¨æ ‡å‡†3x3 Sobelæ ¸è®¡ç®—æ¢¯åº¦ï¼ˆä¿æŒæ¸…æ™°ï¼‰
        grad_x = conv2d(pad(position_map, (1, 1, 1, 1), mode='replicate'), 
                       kernel_x, padding='valid', groups=3)
        grad_y = conv2d(pad(position_map, (1, 1, 1, 1), mode='replicate'), 
                       kernel_y, padding='valid', groups=3)
        
        # è®¡ç®—æ³•çº¿
        cross_product = torch.cross(grad_x, grad_y, dim=1)
        normals = F.normalize(cross_product, dim=1)
        normals[:, 1] *= -1
        
        if output_mode != "Standard":
            normals[:, 0] *= -1
        
        if output_mode == "MiDaS":
            normals = torch.flip(normals, dims=[1,])
        
        normals = normals.movedim(1, -1) * 0.5 + 0.5 # BHWC
        
        return (normals,) 